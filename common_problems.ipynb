{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7560b931-d40f-4397-8614-0f725428a6b5",
   "metadata": {},
   "source": [
    "# Load \n",
    "Loading data and removing whitespace from header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14321c59-75c7-451e-8487-5f2e2cdc0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "samples = spark.read.csv(\n",
    "    os.path.expanduser(\"~/data/DataSample.csv\"),\n",
    "    header=True,\n",
    "    sep=\",\",\n",
    "    inferSchema=\"True\",\n",
    ")\n",
    "points = spark.read.csv(\n",
    "    os.path.expanduser(\"~/data/POIList.csv\"), header=True, sep=\",\", inferSchema=\"True\"\n",
    ")\n",
    "# removing excess whitespace in headers\n",
    "for each in samples.schema.names:\n",
    "    samples = samples.withColumnRenamed(each, each.strip())\n",
    "for each in points.schema.names:\n",
    "    points = points.withColumnRenamed(each, each.strip())\n",
    "samples = samples.withColumn(\"TimeSt\", F.to_timestamp(\"TimeSt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aee30d4-ceab-4f44-b718-abdea1338e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+-------+--------+---------+--------+---------+\n",
      "|_ID    |TimeSt                 |Country|Province|City     |Latitude|Longitude|\n",
      "+-------+-----------------------+-------+--------+---------+--------+---------+\n",
      "|4516516|2017-06-21 00:00:00.143|CA     |ON      |Waterloo |43.49347|-80.49123|\n",
      "|4516547|2017-06-21 18:00:00.193|CA     |ON      |London   |42.9399 |-81.2709 |\n",
      "|4516550|2017-06-21 15:00:00.287|CA     |ON      |Guelph   |43.5776 |-80.2201 |\n",
      "|4516600|2017-06-21 15:00:00.307|CA     |ON      |Stratford|43.3716 |-80.9773 |\n",
      "|4516613|2017-06-21 15:00:00.497|CA     |ON      |Stratford|43.3716 |-80.9773 |\n",
      "+-------+-----------------------+-------+--------+---------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+---------+-----------+\n",
      "|POIID|Latitude |Longitude  |\n",
      "+-----+---------+-----------+\n",
      "|POI1 |53.546167|-113.485734|\n",
      "|POI2 |53.546167|-113.485734|\n",
      "|POI3 |45.521629|-73.566024 |\n",
      "|POI4 |45.22483 |-63.232729 |\n",
      "+-----+---------+-----------+\n",
      "\n",
      "There are 22025 rows in DataSample.csv\n",
      "There are 4 rows in the POIList.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('_ID', 'int'),\n",
       "  ('TimeSt', 'timestamp'),\n",
       "  ('Country', 'string'),\n",
       "  ('Province', 'string'),\n",
       "  ('City', 'string'),\n",
       "  ('Latitude', 'double'),\n",
       "  ('Longitude', 'double')],\n",
       " [('POIID', 'string'), ('Latitude', 'double'), ('Longitude', 'double')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.show(5, truncate=False)\n",
    "points.show(truncate=False)\n",
    "print(f\"There are {samples.count()} rows in DataSample.csv\")\n",
    "print(f\"There are {points.count()} rows in the POIList.csv\")\n",
    "samples.dtypes, points.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2cf638-2262-467c-ab88-137af7b57ea8",
   "metadata": {},
   "source": [
    "# 1. Cleanup\n",
    "\n",
    "Find the sample dataset of request logs in `data/DataSample.csv`. We consider records with identical `geoinfo` and `timest` as suspicious. Please clean up the sample dataset by filtering out those questionable request records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb0d34e-a99c-4f0c-81d6-f65bcf8dee88",
   "metadata": {},
   "source": [
    "# Q1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700ce9f7-9c3e-43d4-b344-057fbe516116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17973 rows in the cleaned DataSample.csv\n"
     ]
    }
   ],
   "source": [
    "# removing all of the duplicated request\n",
    "samples = samples.join(\n",
    "    samples.groupBy(\"Latitude\", \"Longitude\", \"TimeSt\")\n",
    "    .count()\n",
    "    .where(\"count=1\")\n",
    "    .drop(\"count\"),\n",
    "    on=[\"Latitude\", \"Longitude\", \"TimeSt\"],\n",
    ")\n",
    "print(f\"There are {samples.count()} rows in the cleaned DataSample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e26ddc2d-3e0e-43a8-922a-5c830bdc1b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|count|count|\n",
      "+-----+-----+\n",
      "|    2|   20|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# there were 40 instances of duplicated TimeSt that I missed\n",
    "samples.groupBy('TimeSt').count().where('count > 1').groupBy('count').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bca97827-f0c8-498f-a7cc-7ac98ece8ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17933 rows in the cleaned DataSample.csv\n",
      "+------+-----+\n",
      "|TimeSt|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = samples.join(\n",
    "    samples.groupBy(\"TimeSt\")\n",
    "    .count()\n",
    "    .where(\"count=1\")\n",
    "    .drop(\"count\"),\n",
    "    on=[\"TimeSt\"],\n",
    ")\n",
    "print(f\"There are {samples.count()} rows in the cleaned DataSample.csv\")\n",
    "# there were 40 instances of duplicated TimeSt that I missed\n",
    "samples.groupBy('TimeSt').count().where('count > 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eadea087-7943-4e7b-9700-14627766cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates in table POIList.csv too but not mentioned for cleaning, opted to keep them\n",
    "# points = points.join(\n",
    "#     points.groupBy(\"Latitude\", \"Longitude\").count().where(\"count=1\").drop(\"count\"),\n",
    "#     on=[\"Latitude\", \"Longitude\"],\n",
    "# )\n",
    "# print(f\"There are {points.count()} rows in the cleaned POIList.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615057c-9f92-484d-b061-270b636b27fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. Label\n",
    "\n",
    "Assign each *request* (from `data/DataSample.csv`) to the closest (i.e., minimum distance) *POI* (from `data/POIList.csv`).\n",
    "\n",
    "Note: a *POI* is a geographical Point of Interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2f2f2-5813-4499-b9a1-59b4772ca77e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Notes\n",
    "\n",
    "- Need to convert coordinates to radians and use angular distance because the earth isnt flat...\n",
    "\n",
    "### haversine\n",
    "$$d=\\theta r$$ \n",
    "Where **d** is the arc length of the corresponding angle $\\theta$ of sphere with radius **r**\n",
    "Can compute **d** by computing $hav(\\theta)$, which can be computed as the difference of the longitude and latitude of two points defining the arc.\n",
    "\n",
    "With $$\\begin{aligned} h&=hav(\\theta) \\\\\n",
    "&=\\sin^2{\\frac{\\theta}{2}}\\\\\n",
    "&=\\frac{1-\\cos{\\theta}}{2}\\\\\n",
    "&=hav(\\phi_2-\\phi_1)+(1-hav(\\phi_1-\\phi_2)-hav(\\phi_1+\\phi_2))\\cdot hav(\\lambda_2-\\lambda_1) \\end{aligned}$$\n",
    "\n",
    "Where $\\phi$ is the latitude and $\\lambda$ is the longitude\n",
    "\n",
    "Solving for **d**:\n",
    "$$\\begin{aligned} d&=2r\\arcsin{\\sqrt{h}} \\\\\n",
    "&= 2r\\arcsin{\\sqrt{\\sin^2{\\frac{\\phi_2-\\phi_1}{2}}+\\cos{\\phi_1}\\cdot\\cos{\\phi_2}\\cdot\\sin^2{\\frac{\\lambda_2-\\lambda_1}{2}}}} \\end{aligned}$$\n",
    "\n",
    "- Answers have errors that are no better than 0.5% due to variation of earth's radius (needed citation)\n",
    "- $h\\in [0,1]$ for $d\\in \\mathbb{R}$\n",
    "- R=6371 KM\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import DistanceMetric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dist = DistanceMetric.get_metric('haversine')\n",
    "def closest_point(point, points):\n",
    "    \"\"\" Find closest point from a list of points. \"\"\"\n",
    "    return points[dist.pairwise([point], points).argmin()]\n",
    "\n",
    "ds['point'] = [(x, y) for x,y in zip(np.radians(ds['Latitude']), np.radians(ds['Longitude']))]\n",
    "poi['point'] = [(x, y) for x,y in zip(np.radians(poi['Latitude']), np.radians(poi['Longitude']))]\n",
    "ds['closest'] = [closest_point(x, list(poi['point'])) for x in ds['point']]\n",
    "ds = ds.merge(poi[['POIID', 'point']].round(4), how='left', validate = 'm:1',\n",
    "         left_on='closest', right_on='point').drop(['closest', 'point_x', 'point_y'], axis=1)\n",
    "```\n",
    "\n",
    "Pandas implementation relatively easy, need to figure out how to implement in spark without looping over the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40cbcfc-4347-4313-bf71-119824413c42",
   "metadata": {},
   "source": [
    "# Q2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16c75f6a-afa6-4869-9201-d83452be6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be using SQL\n",
    "points.createOrReplaceTempView(\"points\")\n",
    "samples.createOrReplaceTempView(\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fbdf1fc-b5e9-47e1-a9b5-7a75ef4700c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to calculate distance, crossjoin likely not optimal\n",
    "query = \"\"\"\n",
    "SELECT _ID,\n",
    "        p.POIID,\n",
    "        ROUND(2 * 6371 * ASIN(SQRT(POW(SIN((RADIANS(p.Latitude)-RADIANS(s.Latitude)) * 0.5), 2)\n",
    "            +COS(RADIANS(s.Latitude))*COS(RADIANS(p.Latitude))\n",
    "            *POW(SIN((RADIANS(p.Longitude)-RADIANS(s.Longitude)) * 0.5),2))), 5) as dist_km\n",
    "FROM samples s CROSS JOIN \n",
    "(\n",
    "SELECT  POIID,\n",
    "        Latitude,\n",
    "        Longitude\n",
    "FROM points) AS p ON 1=1\n",
    "ORDER BY s._ID\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77028508-2a7a-4f87-bce9-1b0094c950cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/61760132/fastest-way-to-calculate-minimum-haversine-distance-between-lat-long-an-array\n",
    "# nearest neighbor type optimization problem, KDtree?\n",
    "\n",
    "# https://johnlekberg.com/blog/2020-04-17-kd-tree.html\n",
    "# seems like if we construct a KDtree we'll have to collect n exit spark df\n",
    "# maybe storing points as a maptype in samples? not sure how I would implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "736a2b49-869f-4c30-b1a3-adc32ab91f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group request from same location and order them by distance (descending)\n",
    "# assign row number and show only the first (closest) POIID\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy([\"_ID\"]).orderBy(\"dist_km\")\n",
    "\n",
    "# dataframe with each request assigned to the closest POI\n",
    "SAMPLES_ASSIGNED = (\n",
    "    spark.sql(query)\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .where(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    "    .orderBy(\"_ID\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47845d4a-77a0-41d4-8caa-b2394b829243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+\n",
      "|    _ID|POIID|  dist_km|\n",
      "+-------+-----+---------+\n",
      "|5615006| POI3|544.65131|\n",
      "|5614912| POI3| 536.8741|\n",
      "|5614909| POI3|461.99513|\n",
      "|5614801| POI3| 522.0193|\n",
      "|5614689| POI3|674.21123|\n",
      "|5614602| POI3|674.21123|\n",
      "|5614566| POI1|818.27649|\n",
      "|5614548| POI1|143.94017|\n",
      "|5614515| POI1|143.23494|\n",
      "|5614446| POI1|281.13286|\n",
      "|5614377| POI3| 502.6455|\n",
      "|5614303| POI3|503.72753|\n",
      "|5614181| POI1|143.94017|\n",
      "|5614136| POI3| 494.0298|\n",
      "|5614047| POI3|528.93372|\n",
      "|5613968| POI3|489.22103|\n",
      "|5613874| POI3|163.01963|\n",
      "|5613826| POI3|533.36461|\n",
      "|5613790| POI3|549.75028|\n",
      "|5613763| POI3|550.20888|\n",
      "+-------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLES_ASSIGNED.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d03d11a4-8bad-4536-90dd-7aad8a1519a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write assigned requests to new file, optional\n",
    "# SAMPLES_ASSIGNED.write.csv(\n",
    "#     os.path.expanduser(\"~/data/DataSample_Assigned.csv\"),\n",
    "#     header=True,\n",
    "#     sep=\",\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5df6f1c2-c1a0-412c-bb69-3bdd740a7faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " TimeSt    | 2017-06-21 15:00:00.307 \n",
      " Latitude  | 43.3716                 \n",
      " Longitude | -80.9773                \n",
      " _ID       | 4516600                 \n",
      " Country   | CA                      \n",
      " Province  | ON                      \n",
      " City      | Stratford               \n",
      "-RECORD 1----------------------------\n",
      " TimeSt    | 2017-06-21 15:00:00.497 \n",
      " Latitude  | 43.3716                 \n",
      " Longitude | -80.9773                \n",
      " _ID       | 4516613                 \n",
      " Country   | CA                      \n",
      " Province  | ON                      \n",
      " City      | Stratford               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Not going to consider the milisec difference in timestamp as suspicious\n",
    "# since 12 other requests were made during that time\n",
    "spark.sql(\n",
    "    \"\"\" \n",
    "SELECT * \n",
    "FROM samples\n",
    "WHERE _ID=4516600 OR _ID=4516613\n",
    "\"\"\"\n",
    ").show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2634d10-496e-4a79-a375-792ceb6973fe",
   "metadata": {},
   "source": [
    "# 3. Analysis\n",
    "1. For each *POI*, calculate the average and standard deviation of the distance between the *POI* to each of its assigned *requests*.\n",
    "2. At each *POI*, draw a circle (with the center at the POI) that includes all of its assigned *requests*. Calculate the radius and density (requests/area) for each *POI*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e937cfd5-52ae-496e-a01a-b9261dbd08bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+-----------+------------+\n",
      "|POIID|       avg_dist_km|         avg_stdev|max_dist_km|num_requests|\n",
      "+-----+------------------+------------------+-----------+------------+\n",
      "| POI4| 517.4837900000005|1511.9820385829657| 9349.57277|         419|\n",
      "| POI1| 300.6753308918157| 388.3868984762896|11531.82083|        8735|\n",
      "| POI3|451.81352890990354|223.66929456118675| 1474.58096|        8779|\n",
      "+-----+------------------+------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis = SAMPLES_ASSIGNED.groupBy('POIID').agg(F.avg('dist_km').alias('avg_dist_km'), F.stddev('dist_km').alias('avg_stdev'), F.max('dist_km').alias('max_dist_km'), F.count('*').alias('num_requests'))\n",
    "\n",
    "analysis.createOrReplaceTempView(\"analysis\")\n",
    "analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3e47130-9d98-4e3b-b96a-e0f3b6caf2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+---------------------+\n",
      "|POIID|avg_dist_km|avg_stdev |density              |\n",
      "+-----+-----------+----------+---------------------+\n",
      "|POI1 |300.67533  |388.3869  |2.0908229782099422E-5|\n",
      "|POI3 |451.81353  |223.66929 |0.0012851621678767263|\n",
      "|POI4 |517.48379  |1511.98204|1.525740301163449E-6 |\n",
      "+-----+-----------+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis_query = \"\"\"\n",
    "SELECT \n",
    "    POIID,\n",
    "    ROUND(avg_dist_km, 5) AS avg_dist_km,\n",
    "    ROUND(avg_stdev, 5) AS avg_stdev, \n",
    "    num_requests / (PI()*POW(max_dist_km, 2)) AS density\n",
    "FROM\n",
    "    analysis\n",
    "ORDER BY\n",
    "    POIID\n",
    "\"\"\"\n",
    "spark.sql(analysis_query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1f499-12cf-4030-809a-12a97a109db4",
   "metadata": {},
   "source": [
    "# 4. Modelling\n",
    "\n",
    "1. To visualize the popularity of each *POI*, map them to a scale that ranges from -10 to 10. Please provide a mathematical model to implement this, considering extreme cases and outliers. Aim to be more sensitive around the average and give as much visual differentiability as possible.\n",
    "2. **Bonus**: Try to develop reasonable hypotheses regarding *POIs*, state all assumptions, testing steps, and conclusions. Include this as a text file (with the name `bonus`) in your final submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
